# -*- coding: utf-8 -*-
"""context_progetto_Tudisco.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16a-RyilxpvR7CU_1iLTKju_jadf0BPTo

Il presente progetto si propone di sviluppare un sistema di classificazione del testo finalizzato alla categorizzazione delle risposte generate da un chatbot operante nel contesto ospedaliero. L'obiettivo principale è identificare e assegnare in modo accurato una delle etichette predefinite alle risposte del chatbot.

**Obiettivo:**
Il task principale consiste nella classificazione delle risposte del chatbot in base alle etichette assegnate.
"""

!pip install datasets
!pip install -U evaluate
!pip install -U transformers[sentencepiece]
!pip install -U accelerate

import torch
import pandas as pd
from datasets import load_dataset, Dataset, DatasetDict
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.model_selection import train_test_split

raw_dataset = load_dataset("vidhikatkoria/DA_MultiWOZ_hospital")

#esplorazione generale del dataset
raw_dataset

#vengono visualizzate le informazioni relative alla prima istanza di addestramento nel dataset grezzo.
raw_train_dataset = raw_dataset["train"]
raw_train_dataset[0]

#viene esplorata la struttura del dataset grezzo per ottenere informazioni sulle sue features.
raw_train_dataset.features

import numpy as np
import matplotlib.pyplot as plt
labels= np.array(raw_train_dataset["act"])
plt.hist(labels)
plt.xticks(rotation=45)
plt.show()

"""Dal grafico si evince come il **dataset sia fortemente sbilanciato**, inoltre esplorando il dataset si può notare la **quantità limitata di dati disponibili**, soprattutto quando si tratta di classi specifiche.

La necessità di avere un numero significativo di esempi per classe è essenziale per garantire una rappresentazione accurata durante l'addestramento e la valutazione del modello, in quanto non ha molto senso allenare un modello con pochi dati per classe. Per tale motivo, si è scelto di considerare come soglia minima di esempi **25** per classe.

La scarsità di dati in alcune classi avrebbe potuto portare a prestazioni del modello non affidabili e generalizzazioni errate. Data questa situazione, si è scelto di concentrarsi esclusivamente **sulla divisione in set di addestramento e di validazione**.

**L'omissione del set di test** è motivata dal desiderio di massimizzare l'utilizzo dei dati limitati a disposizione per l'addestramento del modello. La divisione in set di addestramento e validazione ci consente di valutare le prestazioni del modello su dati non visti senza ridurre ulteriormente il numero di esempi disponibili per l'addestramento.

Tuttavia, è fondamentale sottolineare che, a causa della mancanza di un set di test separato, le prestazioni del modello sul set di validazione potrebbero non riflettere completamente la sua capacità di generalizzazione su nuovi dati. È importante, quindi, considerare che il modello potrebbe non essere ottimale su dati completamente inediti.

"""

for set_name in raw_dataset.keys():
  raw_dataset[set_name]= raw_dataset[set_name].rename_column("act","labels")

raw_dataset

"""Si è scelto così di effettuare un Downsampling, ovvero la riduzione del numero di esempi di una classe specifica."""

# Creazione del DataFrame per rappresentare il dataset
df = pd.DataFrame(raw_dataset["train"])

# Definizione della soglia minima di esempi per classe
soglia_minima = 25

# Calcolo del numero di esempi disponibili per ciascuna classe nel dataset
conteggio_per_classe = df['labels'].value_counts()

# Vengono selezionate le classi che superano la soglia minima di esempi
classi_selezionate = conteggio_per_classe[conteggio_per_classe >= soglia_minima].index

# Filtraggio del DataFrame originale mantenendo solo le classi selezionate
df_selezionato = df[df['labels'].isin(classi_selezionate)]

# Suddivisione del dataset in set di addestramento e di validazione
# Il 20% del dataset viene assegnato al set di validazione
train_data, validation_data = train_test_split(df_selezionato, test_size=0.2, random_state=42, stratify=df_selezionato['labels'])

# Conversione dei DataFrame risultanti in Dataset
train_dataset = Dataset.from_pandas(train_data)
validation_dataset = Dataset.from_pandas(validation_data)

# Determinazione del numero massimo di esempi per classe (può essere la soglia minima o il numero attuale di esempi nella classe)
min_num_examples = min(conteggio_per_classe[classi_selezionate])
num_examples = min_num_examples

# Inizializzazione di una lista per contenere i vari sub-dataset in un unico dataset
downsampled_data = []

# Viene eseguito il downsampling con sostituzione con lo scopo di bilanciare le classi senza eliminare completamente esempi dalla classe sovrarappresentata
for classe in classi_selezionate:
    classe_data = train_data[train_data['labels'] == classe].sample(num_examples, replace=True, random_state=42)
    downsampled_data.append(classe_data)

# Vengono concatenati i sub-dataset in un unico set di addestramento bilanciato
train_downsampled = pd.concat(downsampled_data)

# Calcolo del numero massimo di esempi per classe nel set di validazione
conteggio_per_classe_val = validation_data['labels'].value_counts()
num_examples_val = min(conteggio_per_classe_val[classi_selezionate])

# Inizializzazione di una lista per contenere i vari sub-dataset in un unico dataset per il set di validazione
downsampled_data_val = []

# Downsampling con sostituzione nel set di validazione
for classe in classi_selezionate:
    classe_data_val = validation_data[validation_data['labels'] == classe].sample(num_examples_val, replace=True, random_state=42)
    downsampled_data_val.append(classe_data_val)

# Concatenazione dei sub-dataset per il set di validazione bilanciato
validation_downsampled = pd.concat(downsampled_data_val)

train_dataset = Dataset.from_pandas(train_downsampled)
validation_dataset = Dataset.from_pandas(validation_downsampled)

# Creazione del DatasetDict con set di addestramento e validazione
combined_dataset_dict = DatasetDict({
    "train": train_dataset,
    "validation": validation_dataset})

combined_dataset_dict

labels= np.array(train_dataset["labels"])
plt.xlabel('Etichette')
plt.ylabel('Numero di esempi')
plt.title('Distribuzione delle etichette nel set di train bilanciato')
plt.hist(labels)
plt.xticks(rotation=45)
plt.show()

labels= np.array(validation_dataset["labels"])
plt.xlabel('Etichette')
plt.ylabel('Numero di esempi')
plt.title('Distribuzione delle etichette nel set di validazione bilanciato')
plt.hist(labels)
plt.xticks(rotation=45)
plt.show()

"""Per pre-elaborare il set di dati, dobbiamo convertire il testo in numeri, in modo tale che il modello possa darne un senso. Questo viene fatto con un tokenizzatore

"""

# Tokenizzazione del testo
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased")

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
label_encoder.fit(combined_dataset_dict["train"]["labels"])

def tokenize_function(examples):
    labels = label_encoder.transform(examples["labels"])
    tokenized_inputs = tokenizer(examples["context"], padding=True, truncation=True, return_tensors="pt")
    tokenized_inputs["labels"] = torch.tensor(labels, dtype=torch.long)

    return tokenized_inputs

mapping_classi = label_encoder.classes_ #contiene le classi del problema di classificazione.
print("Classi presenti:", mapping_classi)

id2label = {0: "Hospital-Inform", 1: "Hospital-Request", 2: "general-bye", 3:"general-thank"}
label2id = {"Hospital-Inform":0, "Hospital-Request":1, "general-bye":2, "general-thank":3}

"""Per mantenere i dati come set di dati, utilizzeremo il metodo: Dataset.map(). Ciò ci consente anche una certa flessibilità extra, se abbiamo bisogno di più pre-elaborazione rispetto alla semplice tokenizzazione. Il metodo map() funziona applicando una funzione su ciascun elemento del set di dati, quindi definiamo una funzione che tokenizza i nostri input. Questa funzione prende un dizionario (come gli elementi del nostro set di dati) e restituisce un nuovo dizionario con le chiavi input_ids, attention_maske token_type_ids"""

tokenized_dataset=combined_dataset_dict.map(tokenize_function, batched=True, remove_columns=["domain", "response", "speaker"])

tokenized_dataset

"""Dobbiamo definire una funzione che applicherà la corretta quantità di riempimento agli elementi del set di dati che vogliamo raggruppare insieme. Fortunatamente, la libreria  Transformers ci fornisce tale funzione tramite file DataCollatorWithPadding. Richiede un tokenizzatore quando ne istanzia (per sapere quale token di riempimento utilizzare e se il modello prevede che il riempimento sia a sinistra o a destra degli input)."""

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Creazione del modello
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-cased", num_labels=len(label_encoder.classes_),id2label=id2label)

import evaluate
import numpy as np

"""**Compute_metrics()** è funzione per calcolare una metrica durante la valutazione del train (altrimenti la valutazione avrebbe semplicemente stampato la perdita, che non è un numero molto intuitivo)."""

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_preds):
    metrics = evaluate.combine([
        evaluate.load("precision"),
        evaluate.load("recall"),
        evaluate.load("f1")])

    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    accuracy = accuracy_score(y_true=labels, y_pred=predictions)
    metrics_dict = metrics.compute(predictions=predictions, references=labels, average="macro")
    metrics_dict['accuracy'] = accuracy
    return metrics_dict

"""il primo passo prima di poter definire il nostro Trainer è definire una **TrainingArgumentsclasse** che conterrà tutti gli **iperparametri** che verranno utilizzati per l'addestramento e la valutazione. L'unico argomento che deve essere fornito è una **directory** in cui verrà salvato il modello addestrato, nonché i **checkpoint** lungo il percorso."""

!pip install wandb
import wandb

# Definizione degli iperparametri per il primo allenamento
training_args_1 = TrainingArguments(
    output_dir="CONTEXT_one",
    evaluation_strategy="steps",
    logging_steps=30,
    eval_steps=30,
    save_strategy="steps",
    learning_rate=2e-5,
    weight_decay=0.02,
    num_train_epochs=30)

trainer= Trainer(
    model= model,
    args=training_args_1,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics)

trainer.train()

"""Al posto delle **epoch** ci si è affidati ai **logging step** come strategia di valutazione.

Questa scelta è stata guidata dalla volontà di monitorare le performance del modello in modo più dettagliato durante l'addestramento, ritenendo che la valutazione a intervalli di passi specifici avrebbe fornito una visione più approfondita del comportamento del modello.

I risultati del trainer sono stati monitorati ad intervalli regolari di 30 passaggi.

*   La training loss nonostante inizialmente sia instabile, diminuisce nel corso dell'allenamento.
*   La validation loss è altalenante. Questo potrebbe essere segno di overfitting
"""

from huggingface_hub import notebook_login

notebook_login()

trainer.push_to_hub()

text1="User: I am looking for the nearest hospital. <SEP> Agent: Ok Addenbrookes hospital is located at Hills Road, Cambridge. Do you want the phone number? <SEP> User: Yes, please. And the postcode."

from transformers import pipeline
classifier=pipeline("text-classification", model="CONTEXT_one")
classifier(text1)

text2=  "Can I have the address please."

from transformers import pipeline
classifier=pipeline("text-classification", model="CONTEXT_one")
classifier(text2)

#le preizioni vengono eseguite sul set di validazione, in quanto manca il set di addestramento
predictions_primo = trainer.predict(tokenized_dataset["validation"])
print(predictions_primo.predictions.shape, predictions_primo.label_ids.shape)


etichette_primo_vere = predictions_primo.label_ids
etichette_primo_predette = np.argmax(predictions_primo.predictions, axis=-1)

import random
i = random.randint(0, 76)
example = tokenized_dataset['validation'][i]['input_ids'] # dal dataset di validation prendo un campione i a caso di validation e stampo i token
example_label = tokenized_dataset['validation'][i]['labels'] # stampo la label dello stesso campione a caso (la label è quella originale, non quella predetta dal modello)

tokenizer.decode(example) # stampo il campione estratto a caso

example_label

predictions_primo.predictions[i].argmax() # ha predetto la stessa cosa del modello

from sklearn.metrics import precision_score
precision = precision_score(etichette_primo_vere, etichette_primo_predette, average="weighted")
print("Precision Score:", precision)

from sklearn.metrics import f1_score
f1 = f1_score(etichette_primo_vere, etichette_primo_predette, average="weighted")
print("F1 Score:", f1)

accuracy1 = accuracy_score(etichette_primo_vere, etichette_primo_predette)
print("accuracy Score:", accuracy1)

from sklearn.metrics import recall_score
recall = recall_score(etichette_primo_vere, etichette_primo_predette, average="weighted")
print("recall score:", recall)